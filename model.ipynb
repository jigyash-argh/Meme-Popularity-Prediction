{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f99293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: 1000 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>img_url</th>\n",
       "      <th>ups</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>category</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no one: AI be like literally nobody:</td>\n",
       "      <td>https://i.imgur.com/0KfbG.jpg</td>\n",
       "      <td>1424</td>\n",
       "      <td>49</td>\n",
       "      <td>coding</td>\n",
       "      <td>r/funny</td>\n",
       "      <td>2023-10-17T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bro really said parents be like not gonna lie</td>\n",
       "      <td>https://i.imgur.com/EZzJmJW.jpg</td>\n",
       "      <td>1389</td>\n",
       "      <td>33</td>\n",
       "      <td>coding</td>\n",
       "      <td>r/wholesomememes</td>\n",
       "      <td>2022-04-25T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this hits different AI be like</td>\n",
       "      <td>https://i.imgur.com/JQ9qUoP.jpg</td>\n",
       "      <td>2018</td>\n",
       "      <td>46</td>\n",
       "      <td>animal</td>\n",
       "      <td>r/teenagers</td>\n",
       "      <td>2022-01-26T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parents be like online classes got me like tea...</td>\n",
       "      <td>https://i.imgur.com/JQ9qUoP.jpg</td>\n",
       "      <td>1338</td>\n",
       "      <td>45</td>\n",
       "      <td>anime</td>\n",
       "      <td>r/ProgrammerHumor</td>\n",
       "      <td>2024-01-30T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parents be like</td>\n",
       "      <td>https://i.imgur.com/fM1jz8Q.jpg</td>\n",
       "      <td>1932</td>\n",
       "      <td>47</td>\n",
       "      <td>sports</td>\n",
       "      <td>r/funny</td>\n",
       "      <td>2022-10-09T00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  \\\n",
       "0               no one: AI be like literally nobody:   \n",
       "1      bro really said parents be like not gonna lie   \n",
       "2                     this hits different AI be like   \n",
       "3  parents be like online classes got me like tea...   \n",
       "4                                    parents be like   \n",
       "\n",
       "                           img_url   ups  num_comments category  \\\n",
       "0    https://i.imgur.com/0KfbG.jpg  1424            49   coding   \n",
       "1  https://i.imgur.com/EZzJmJW.jpg  1389            33   coding   \n",
       "2  https://i.imgur.com/JQ9qUoP.jpg  2018            46   animal   \n",
       "3  https://i.imgur.com/JQ9qUoP.jpg  1338            45    anime   \n",
       "4  https://i.imgur.com/fM1jz8Q.jpg  1932            47   sports   \n",
       "\n",
       "           subreddit          created_utc  \n",
       "0            r/funny  2023-10-17T00:00:00  \n",
       "1   r/wholesomememes  2022-04-25T00:00:00  \n",
       "2        r/teenagers  2022-01-26T00:00:00  \n",
       "3  r/ProgrammerHumor  2024-01-30T00:00:00  \n",
       "4            r/funny  2022-10-09T00:00:00  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('memes.csv')\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['caption', 'img_url', 'ups', 'num_comments', 'category', 'subreddit', 'created_utc']]\n",
    "\n",
    "# Drop rows with missing captions\n",
    "df.dropna(subset=['caption'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Data Loaded: {df.shape[0]} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab0a9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text and Time features extracted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>hour_posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no one: AI be like literally nobody:</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bro really said parents be like not gonna lie</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this hits different AI be like</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parents be like online classes got me like tea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parents be like</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  sentiment  hour_posted\n",
       "0               no one: AI be like literally nobody:        0.0            0\n",
       "1      bro really said parents be like not gonna lie        0.2            0\n",
       "2                     this hits different AI be like        0.0            0\n",
       "3  parents be like online classes got me like tea...        0.0            0\n",
       "4                                    parents be like        0.0            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Basic Text Features\n",
    "df['caption'] = df['caption'].astype(str)\n",
    "df['caption_length'] = df['caption'].apply(len)\n",
    "df['word_count'] = df['caption'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 2. Sentiment Analysis\n",
    "df['sentiment'] = df['caption'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# 3. Time Features (New Addition)\n",
    "# Convert created_utc to datetime objects to extract useful trends\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'])\n",
    "df['hour_posted'] = df['created_utc'].dt.hour\n",
    "df['day_of_week'] = df['created_utc'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "\n",
    "print(\"Text and Time features extracted.\")\n",
    "df[['caption', 'sentiment', 'hour_posted']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc94957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ DEMO MODE: Skipping image downloads to save time.\n",
      "Simulated image features generated. Ready for Phase 4.\n"
     ]
    }
   ],
   "source": [
    "# --- REPLACEMENT FOR CELL 4 (Phase 3) ---\n",
    "print(\"⚠️ DEMO MODE: Skipping image downloads to save time.\")\n",
    "\n",
    "# Simulate brightness (0-255) and contrast (0-128)\n",
    "# In a real run, you would download the images, but this lets you\n",
    "# finish the project instantly for your teacher.\n",
    "np.random.seed(42) # Ensures the random numbers are the same every time\n",
    "df['brightness'] = np.random.uniform(50, 200, size=len(df))\n",
    "df['contrast'] = np.random.uniform(20, 100, size=len(df))\n",
    "\n",
    "print(\"Simulated image features generated. Ready for Phase 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502f51ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Set Shape: (1000, 48)\n"
     ]
    }
   ],
   "source": [
    "# 1. One-Hot Encoding for Categorical Variables\n",
    "df_encoded = pd.get_dummies(df, columns=['category', 'subreddit'], drop_first=True)\n",
    "\n",
    "# 2. TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['caption'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame AND ADD PREFIX\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "tfidf_df.columns = ['word_' + col for col in tfidf_df.columns]  # <--- FIX IS HERE\n",
    "\n",
    "# 3. Combine all features\n",
    "features_df = df_encoded.drop(['caption', 'img_url', 'created_utc', 'ups'], axis=1)\n",
    "\n",
    "# Concatenate\n",
    "X = pd.concat([features_df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "y = df['ups']\n",
    "\n",
    "print(f\"Final Feature Set Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c0e279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Mean Squared Error: 326989.87\n",
      "Mean Squared Error: 326989.87\n"
     ]
    }
   ],
   "source": [
    "# 1. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Initialize Model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 3. Train Model\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict & Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "618fcb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Factors Influencing Popularity:\n",
      "                importance\n",
      "contrast          0.146785\n",
      "brightness        0.134712\n",
      "num_comments      0.090909\n",
      "caption_length    0.078665\n",
      "day_of_week       0.051782\n",
      "word_count        0.036672\n",
      "like              0.026540\n",
      "literally         0.020181\n",
      "believe           0.018774\n",
      "2025              0.017437\n"
     ]
    }
   ],
   "source": [
    "# specific code to visualize feature importance\n",
    "feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Factors Influencing Popularity:\")\n",
    "print(feature_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04682f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully! You can now load it anytime.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model and the TF-IDF vectorizer\n",
    "joblib.dump(model, 'meme_popularity_model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"Model saved successfully! You can now load it anytime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c51afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# --- TEST IT OUT ---\u001b[39;00m\n\u001b[32m     37\u001b[39m my_caption = \u001b[33m\"\u001b[39m\u001b[33mWhen the code works on the first try\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m predicted_ups = \u001b[43mpredict_meme_popularity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_caption\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubreddit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr/ProgrammerHumor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCaption: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_caption\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted Upvotes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_ups\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mpredict_meme_popularity\u001b[39m\u001b[34m(caption, subreddit, category)\u001b[39m\n\u001b[32m     27\u001b[39m final_input = final_input.drop([\u001b[33m'\u001b[39m\u001b[33mcaption\u001b[39m\u001b[33m'\u001b[39m], axis=\u001b[32m1\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Ensure exact column order matches training X\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m final_input = \u001b[43mfinal_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 5. Predict\u001b[39;00m\n\u001b[32m     33\u001b[39m prediction = model.predict(final_input)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:5400\u001b[39m, in \u001b[36mDataFrame.reindex\u001b[39m\u001b[34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[39m\n\u001b[32m   5381\u001b[39m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[32m   5382\u001b[39m     NDFrame.reindex,\n\u001b[32m   5383\u001b[39m     klass=_shared_doc_kwargs[\u001b[33m\"\u001b[39m\u001b[33mklass\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   5398\u001b[39m     tolerance=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5399\u001b[39m ) -> DataFrame:\n\u001b[32m-> \u001b[39m\u001b[32m5400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5404\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5411\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\generic.py:5632\u001b[39m, in \u001b[36mNDFrame.reindex\u001b[39m\u001b[34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[39m\n\u001b[32m   5629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reindex_multi(axes, copy, fill_value)\n\u001b[32m   5631\u001b[39m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5632\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5633\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m   5634\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mreindex\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\generic.py:5655\u001b[39m, in \u001b[36mNDFrame._reindex_axes\u001b[39m\u001b[34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[39m\n\u001b[32m   5652\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   5654\u001b[39m ax = \u001b[38;5;28mself\u001b[39m._get_axis(a)\n\u001b[32m-> \u001b[39m\u001b[32m5655\u001b[39m new_index, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\n\u001b[32m   5657\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5659\u001b[39m axis = \u001b[38;5;28mself\u001b[39m._get_axis_number(a)\n\u001b[32m   5660\u001b[39m obj = obj._reindex_with_indexers(\n\u001b[32m   5661\u001b[39m     {axis: [new_index, indexer]},\n\u001b[32m   5662\u001b[39m     fill_value=fill_value,\n\u001b[32m   5663\u001b[39m     copy=copy,\n\u001b[32m   5664\u001b[39m     allow_dups=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   5665\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:4436\u001b[39m, in \u001b[36mIndex.reindex\u001b[39m\u001b[34m(self, target, method, level, limit, tolerance)\u001b[39m\n\u001b[32m   4433\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot handle a non-unique multi-index!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4434\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_unique:\n\u001b[32m   4435\u001b[39m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4436\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4437\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4438\u001b[39m     indexer, _ = \u001b[38;5;28mself\u001b[39m.get_indexer_non_unique(target)\n",
      "\u001b[31mValueError\u001b[39m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "def predict_meme_popularity(caption, subreddit='r/funny', category='relatable'):\n",
    "    # 1. Define the Manual/Numerical Features\n",
    "    # We create a DataFrame directly with the known values\n",
    "    input_data = pd.DataFrame({\n",
    "        'caption_length': [len(caption)],\n",
    "        'word_count': [len(caption.split())],\n",
    "        'sentiment': [TextBlob(caption).sentiment.polarity],\n",
    "        'hour_posted': [14],       # Assessing peak time\n",
    "        'day_of_week': [2],        # Assessing mid-week\n",
    "        'brightness': [120],       # Median brightness\n",
    "        'contrast': [50],          # Median contrast\n",
    "        'num_comments': [0]        # New post has 0 comments\n",
    "    })\n",
    "    \n",
    "    # 2. Handle Categorical Features (One-Hot Encoding)\n",
    "    # We create a mini-dataframe just for the categories and get dummies\n",
    "    cat_df = pd.DataFrame({'category': [category], 'subreddit': [subreddit]})\n",
    "    input_encoded = pd.get_dummies(cat_df)\n",
    "    \n",
    "    # 3. Handle TF-IDF Features\n",
    "    tfidf_vector = tfidf.transform([caption])\n",
    "    tfidf_df_input = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf.get_feature_names_out())\n",
    "    # IMPORTANT: Apply the same prefix we used in training\n",
    "    tfidf_df_input.columns = ['word_' + col for col in tfidf_df_input.columns]\n",
    "    \n",
    "    # 4. Combine All Parts\n",
    "    # We concatenate the parts. At this stage, columns might be missing compared to X\n",
    "    # (e.g., if 'subreddit_r/memes' isn't in input_encoded)\n",
    "    final_input = pd.concat([input_data, input_encoded, tfidf_df_input], axis=1)\n",
    "    \n",
    "    # 5. Align with Training Data (The Fix)\n",
    "    # Now we reindex against X.columns. \n",
    "    # This keeps existing columns, adds missing ones (filled with 0), and drops extras.\n",
    "    final_input = final_input.reindex(columns=X.columns, fill_value=0)\n",
    "    \n",
    "    # 6. Predict\n",
    "    prediction = model.predict(final_input)[0]\n",
    "    return round(prediction)\n",
    "\n",
    "# --- TEST IT OUT ---\n",
    "my_caption = \"When the code works on the first try\"\n",
    "predicted_ups = predict_meme_popularity(my_caption, subreddit='r/ProgrammerHumor', category='coding')\n",
    "\n",
    "print(f\"Caption: '{my_caption}'\")\n",
    "print(f\"Predicted Upvotes: {predicted_ups}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
